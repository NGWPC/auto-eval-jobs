# begin interfaces/interfaces.md
# Interface definitions for FIM Evaluation Jobs Arguments, Inputs, and Outputs

This interface directory contains a set of yaml files that specifies interfaces for the jobs that make up the HAND FIM evaluation pipeline. The pipeline has been designed as a series of chained jobs that will be run by a batch processing solution. The primary focus of this repo is on describing inputs and outputs of each job and each jobs arguments/parameters. 

Below we give a human readable description of the contents of each yaml file. The descriptions also expand on the anticipated behavior of the jobs for common input and argument combinations. 

By convention when outputs are listed for a job it is assumed that these outputs will always be written to a filepath(s) that is specified when a job is called. This is to make it easier to integrate the jobs with a batch orchestrator. std-out is reserved for job logging rather than output. 

## HAND Inundator (`hand_inundator`)

**Implementation status: Inundated extents implemented. Depth FIM production will be added in FY26**

### Example docker run command

From inside the inundate-dev container would run:

```
python inundate.py --catchment_data_path /path/to/catchment/json/ --forecast_path /path/to/forecast/ --fim_output_path /path/to/output/ --fim_type extent
```

This example lists all possible arguments. See yaml files for optional vs required arguments.

### Description  
- Generates flood extent/depth maps from a HAND REM. This job inundates a *single* hand catchment. It can be configured to return either a depth FIM or an extent FIM.

### Arguments  
- **fim_type**
  - Extent (binary) vs Depth (float values)  

### Inputs 
- **catchment_data_path**:
  - This input is path to a JSON file that contains a rating curve every HydroID in a HAND catchment along with metadata necessary to process the HydroID. The json file should have the following structure:
  ```json
  {
      "<catchment_id>": {
        "hydrotable_entries": {
          "<HydroID>": {
            "stage": ["array_of_stage_values"],
            "discharge_cms": ["array_of_discharge_values"],
            "nwm_feature_id": "<integer>",
            "lake_id": "<integer>"
          }
          // More HydroID entries...
        },
        "raster_pair": {
          "rem_raster_path": "<path_value>",
          "catchment_raster_path": "<path_value>"
        }
      }
  }
  ```
  - The **raster_pair** lists paths to the two rasters that are used to generate the HAND extent.
    - **rem_raster_path**
    - This is a path to a HAND relative elevation tiff for this catchment. This would typically be an s3 path but could be a local filepath as well. 
    - **catchment_raster_path**
    - This is a path to a tiff that helps map every location in the catchment to a rating curve associated with that location. Every pixel is assigned an integer value that reflects the HydroID of the sub-catchment it is in. This value can then be used to look up an associated rating curve in the hydrotable_entries object inside the catchment json. This rating curve is used to interpolate a stage value for a given NWM reach discharge. If the stage value is larger than the HAND value at that pixel then the pixel is marked flooded.
- **forecast_path**
  - A path to a csv file listing NWM feature_id values and their respective discharges. A stage is obtained for these discharges for each HydroID catchment by using the rating associated with that HydroID.

### Outputs 
- **fim_output_path**
  - This is a depth or extent raster generated from the HAND data depending on the value of the fim_type argument. The format of this raster is specified in `hand_inundator.yml'

---

## Mosaic Maker (`fim_mosaicker`) 

**Implementation status: Partially implemented. The depth FIM functionality will be implemented in FY26.**

### Example command

From inside the mosaic-dev container would run:

```
python mosaic.py --raster_paths /paths/to/rasters/ --hwm_paths /path/to/multipoint/geometries --mosaic_output_path /path/to/output/ --clip_geometry /path/to/clipvectors --fim_type extent
```

This example lists all possible arguments. See yaml files for optional vs required arguments.

### Description  
This job mosaics flood extents and benchmark raster data from either HAND or benchmark sources using a pixel-wise NAN-MAX selection policy. That is, for all the images being mosaicked if there are overlapping raster pixels then the maximum value of the overlapping rasters at that pixel location is selected. No-Data values are not considered when selecting the maximum (they are treated as Nan) unless all the pixels are No-Data. Rasters can be either depth or extent rasters and the mosaicking policy for overlapping rasters will remain the same. The resolution of the produced raster will be determined by the lowest resolution raster in the input data.

### Arguments
- **fim_type**
  - This informs the job whether it is mosaicking FIMs with extents or depths.

### Inputs
- **raster_paths** and/or **hwm_paths**: 
  - Array of paths to TIFF/GeoJSON/GeoPackage rasters and/or vector files. If a vector is listed in the array then the output will be a vector. 
- **clip_geometry_path**
  - Optional path to a GeoJSON or gpkg file with a boundary to clip the mosaicked output to. This input will always be given in the HAND FIM evaluation pipeline and will describe the ROI being evaluated.

### Outputs 
- **mosaic_output_path**
  - **Raster**
    - In the case of raster output, the output will be a path pointing to a single mosaicked raster.
  - **Vector** 
    - In the case of vector output, the output will be a path pointing to a single mosaicked vector gpkg.

---

## Agreement Maker (`agreement_maker`) 

**Implementation status:  Will be implemented in NGWPC PI-6. The depth FIM functionality will be implemented in FY26.**

### Example command

From inside the agreement-dev container would run:

```
python agreement.py --benchmark_path /path/to/raster/ --candidate_path /path/to/raster/ --agreement_path /path/to/agreement/ --clip_geoms /path/to/clipdictionary --fim_type extent 
```

This example lists all possible arguments. See yaml files for optional vs required arguments.

**Note on implementation memory usage:** The inundate and mosaicker jobs limit the memory used for raster processing by setting the GDAL_CACHEMAX environment variable. If the rioxarray based GVAL is used for the metrics_calculator job then a different argument or arguments will be needed to constrain the memory usage of the raster handling involved in the metrics calculation. If GVAL can't be made to limit its memory usage we will need to pursue a different approach.

### Description  
Creates an agreement map showing where a pair of input rasters spatially concur. The job works with depth or extent data with the assumption that a given pair will be either both depths or extents. Produces either a continuous agreement map when the inputs are depths or a categorical agreement map for extents. The resolution of the produced raster will be determined by the lowest resolution raster in the input data.


### Arguments  

- **fim_type**
  - Specifies whether agreement is based on spatial 'extent' (agreement between binary categorical rasters) or between rasters with depth values. Influences output raster format.
 
### Inputs
- **benchmark_path**:  
  - path to depth or extent raster benchmark data.  

- **candidate_path**:  
  - path to depth or extent raster benchmark data.   

- **clip_geoms**
  - This is an optional path to json file that that includes paths to geopackage of masks to exclude or include in the final produced agreement. The input format is identical to the previous format that was previously used to mask areas over which to evaluate FIM model skill. Each mask geometry can also be buffered by setting a buffer flag to an integer value (with units of meters) in the sub-dictionaries "buffer" key.

  ```json
  {
    "levees": {
      "path": "path/to/levee/file",
      "buffer": null,
      "operation": "exclude"
    },
    "waterbodies": {
      "path": "path/to/waterbody/file",
      "buffer": null,
      "operation": "exclude"
    }
  }
  ```
  
### Outputs 
Output is a single raster 
- **agreement_path**
    - See `agreement_maker.yml` for a description of the output raster format for continuous or categorical agreement rasters.

---

## HWM Agreement Maker (`hwm_agreement`) 

**Implementation status:  Will be implemented in NGWPC PI-6**

### Example command

From inside the agreement-dev container would run:

```
python agreement.py --benchmark_path /path/to/multipoint --candidate_path /path/to/raster --agreement_path /path/to/agreement/ --clip_geoms /path/to/clipdictionary --fim_type extent 
```

This example lists all possible arguments. See yaml files for optional vs required arguments.

### Description  
Creates an agreement multipoint geometry showing where a FIM raster and a set of HWM points associated with an event spatially concur. The job works with depth or extent rasters with the assumption that a given HWM survey will the attributes required to produce an agreement map. The agreement geometry will be the same HWM point geometry with attributes indicating agreement between the HWM points and the raster being compared.

### Arguments  

- **fim_type**
  - Specifies whether agreement is based on spatial 'extent' overlap (binary) or potentially 'depth' values (requires specific logic in the script). Influences output raster format.
 
### Inputs
- **benchmark_path**:  
  - path to vector (as geopackage) benchmark data. ~must be either a point or multipoint geometry.  

- **candidate_path**:  
  - path to vector (as geopackage) benchmark data. If a vector must be either a point or multipoint geometry.  

- **clip_geoms**
  - This is an optional path to json file that that includes paths to geopackage of masks to exclude or include in the final produced agreement. The input format is identical to the previous format that was previously used to mask areas over which to evaluate FIM model skill. Each mask geometry can also be buffered by setting a buffer flag to an integer value (with units of meters) in the sub-dictionaries "buffer" key.

  ```json
  {
    "levees": {
      "path": "path/to/levee/file",
      "buffer": null,
      "operation": "exclude"
    },
    "waterbodies": {
      "path": "path/to/waterbody/file",
      "buffer": null,
      "operation": "exclude"
    }
  }
  ```
  
### Outputs 
Output is a geopackage of vector information.
- **agreement_path**
    - See `agreement_maker.yml` for a description of output vector format. The returned geopackage could have additional attributes that are passed through from the input vector data to the output data. 

---

## Metrics Calculator (`metrics_calculator`) 

**Implementation status:  Will be implemented in NGWPC PI-6**


### Example command

From inside the metrics-dev container would run command below:

```
python metrics.py --agreement_path /path/to/agreement/ --metrics_path /path/to/metrics/json
```

This example lists all possible arguments. See yaml files for optional vs required arguments.

**Note on implementation memory usage:** The inundate and mosaicker jobs limit the memory used for raster processing by setting the GDAL_CACHEMAX environment variable. If the rioxarray based GVAL is used for the metrics_calculator job then a different argument or arguments will be needed to constrain the memory usage of the raster handling involved in the metrics calculation. If GVAL can't be made to limit its memory usage we will need to pursue a different approach.

### Description  
This job is designed to take an agreement map raster and calculate summary metrics of the agreement of two FIMs over a given ROI.

### Arguments  


### Input  
- **agreement_path**
  - Path to an agreement raster over which the metrics will be calculated.

### Output  
- **metrics_path**
  - The output will be a json file containing the metrics the user requested. `metrics_calculator.yml` lists a small subset of possible metrics.

---

##  HWM Metrics Calculator (`hwm_metrics`) 

### Example command

From inside the metrics-dev container would run command below:

```
python metrics.py --agreement_path /path/to/agreement/ --metrics_path /path/to/metrics/json
```

This example lists all possible arguments. See yaml files for optional vs required arguments.

### Description  
This job is designed to take an agreement map raster and calculate summary metrics of the agreement of two FIMs over a given ROI.

### Arguments  


### Input  
- **agreement_path**
  - Path to an agreement gpkg containing a multipoint geometry with the attributes over which the metrics will be calculated.

### Output  
- **metrics_path**
  - The output will be a json file containing the metrics the user requested. `metrics_calculator.yml` lists a small subset of possible metrics.
---
# begin interfaces/job_conventions.md
## Input and outputs
All data inputs and outputs from containers should be written to and read from files. All non-geospatial IO should always use the python [fsspec library](https://filesystem-spec.readthedocs.io/en/latest/) to read and write a jobs terminal inputs and outputs. For geospatial data, IO will sometimes use GDAL's drivers instead of fsspec to flexibly write to either local or cloud storage because many geospatial libraries can use these drivers when reading and writing to the cloud.

## Entrypoint script arguments

### names and abbreviations

* If an argument is a path then full argument name should have “path” at the end.
* If two or more jobs accept the same argument then the argument name and abbreviation should be the same across all the jobs. 
* If an argument has the same name across jobs then it should also have the same abbreviation.

## Logging

### Log format
All log entries will be a single JSON object per line. The log entries should the following keys:

* timestamp
* level
* job_id
* message

An example log entry might be:

```
{
  "timestamp": "2025-04-14T16:10:15.543210Z",
  "level": "INFO",
  "job_id": "hand_inundator",
  "message": "Began inundate job for branch 0",
}
```

### Log levels

Below are guidelines for what to include at each log level:

* Debug: Log fine grained detail about job execution here. Would typically be used for debugging.
* Info: Log messages at this level that track job progression at a high level.
* Warning: Log non-breaking errors here that indicate an out of the ordinary exception occured that was handled.
* Error: Log errors that result in a run failure here.
* Success: A message recording a successful run. Success log messages will also contain keys that correspond to the output paths of a given job so that a successful data write with pathing is recorded.

### Results

When a job runs successfully then the last message should look like this:

```
{
  "timestamp": "2025-04-14T16:10:15.543210Z",
  "level": "SUCCESS",
  "job_id": "hand_inundator",
  "message": {
    "output_type1": "s3://fimc-data/path/to/filetype1",
    "output_type2": "s3://fimc-data/path/to/file2"
  }
}
```

The message of a success log is a json object whose keys are strings containing filepaths or lists of strings containing filepaths.

### Error logging

If an error message is logged, **it should always be the last message logged by the job**. Error messages will follow a consistent format across jobs. Here is an example:

```
{
  "timestamp": "2025-04-14T16:10:15.543210Z",
  "level": "ERROR",
  "job_id": "hand_inundator",
  "message": "Inundate job run failed: {fatal error message here}",
}
```

### Logging libraries

The logging module from the python standard library will be used to log messages to stderr. The library python-json-logger will be used to create a formatting object to format the log messages.
# begin example.env
AWS_DEFAULT_REGION=us-east-1
GDAL_CACHEMAX=1024
# begin fim_mosaicker/mosaic.py
import rasterio
from rasterio.merge import merge
from rasterio.windows import Window
from rasterio.mask import mask
import numpy as np
from typing import Union, Optional, Literal
import os
import sys
import fiona
import argparse
from pathlib import Path


def mosaic_rasters(
    raster_paths: list[str],
    output_path: str,
    clip_geometry: Optional[Union[str, dict]] = None,
    fim_type: Literal["depth", "extent"] = "depth",
    geo_mem_cache: int = 256,  # Control GDAL cache size in MB
) -> str:
    """Raster mosaicking using rasterio with optimized memory settings.

    Parameters
    ----------
    raster_paths : list[str]
        List of paths to rasters to be mosaicked.
    output_path : str
        Path where to save the mosaicked output.
    clip_geometry : str or dict, optional
        Vector file path or GeoJSON-like geometry to clip the output raster.
    fim_type : str, optional
        Type of FIM output, either "depth" or "extent".
        For depth: uses float32 dtype and -9999 as nodata.
        For extent: uses uint8 dtype and 255 as nodata, converts all nonzero values to 1.
    geo_mem_cache : int, optional
        GDAL cache size in megabytes, by default 256 MB.
        Controls memory usage during raster processing.

    Returns
    -------
    str
        Path to the output raster.

    Raises
    ------
    ValueError
        If input parameters are invalid or no rasters provided.
    RuntimeError
        If unable to open input files or process data.
    """
    if fim_type not in ["depth", "extent"]:
        raise ValueError("fim_type must be either 'depth' or 'extent'")

    if not raster_paths:
        raise ValueError("No rasters provided for mosaicking.")

    # Set raster properties based on fim_type
    if fim_type == "depth":
        nodata = -9999
        dtype = "float32"
    else:  # extent
        nodata = 255
        dtype = "uint8"

    # Enhanced GDAL environment settings for better performance
    config_options = {
        "GDAL_CACHEMAX": geo_mem_cache,
        "VSI_CACHE_SIZE": 1024 * 1024 * min(256, geo_mem_cache),  # VSI cache in bytes
        "GDAL_DISABLE_READDIR_ON_OPEN": "TRUE",  # Performance boost for S3/cloud storage
        "CPL_VSIL_CURL_ALLOWED_EXTENSIONS": ".tif,.vrt",  # Limit allowed extensions
    }

    # Open all rasters and get their metadata
    src_files = []
    try:
        with rasterio.Env(**config_options):
            for path in raster_paths:
                src = rasterio.open(path)
                src_files.append(src)

            # Check that all rasters have the same CRS
            crs = src_files[0].crs
            if not all(src.crs == crs for src in src_files):
                raise ValueError("All rasters must have the same CRS")

            # Get bounds of the mosaic
            bounds = [src.bounds for src in src_files]
            left = min(bound.left for bound in bounds)
            bottom = min(bound.bottom for bound in bounds)
            right = max(bound.right for bound in bounds)
            top = max(bound.top for bound in bounds)

            # Get resolution (assuming all rasters have same resolution)
            res = src_files[0].res
            if not all(src.res == res for src in src_files):
                raise ValueError("All rasters must have the same resolution")

            # Calculate output dimensions
            width = int((right - left) / res[0] + 0.5)
            height = int((top - bottom) / res[1] + 0.5)

            # Create output profile
            profile = src_files[0].profile.copy()
            profile.update(
                {
                    "driver": "GTiff",
                    "height": height,
                    "width": width,
                    "transform": rasterio.transform.from_bounds(
                        left, bottom, right, top, width, height
                    ),
                    "dtype": dtype,
                    "nodata": nodata,
                    "tiled": True,
                    "blockxsize": 256,  # Standard block size for efficient processing
                    "blockysize": 256,
                    "compress": "lzw",
                    "predictor": 2,
                }
            )

            # Create output raster
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with rasterio.open(output_path, "w", **profile) as dst:
                # Get a list of all blocks for processing
                windows = list(dst.block_windows(1))

                # Process each window
                for idx, (_, window) in enumerate(windows):
                    # Create a transform for this particular window
                    window_transform = dst.window_transform(window)

                    # Initialize output block with nodata
                    out_data = np.full(
                        (window.height, window.width), nodata, dtype=profile["dtype"]
                    )

                    # Track if we've found any valid data for this window
                    has_valid_data = False

                    # Compute the window bounds in coordinate space
                    window_bounds = rasterio.transform.array_bounds(
                        window.height, window.width, window_transform
                    )

                    # For each source raster, check if it overlaps with this window
                    for src in src_files:
                        # Calculate the window in the source raster's coordinate system
                        src_window = src.window(*window_bounds)

                        # Check if the source overlaps this window
                        if src_window.width <= 0 or src_window.height <= 0:
                            continue

                        # Read data from the source raster
                        try:
                            # Ensure the window is valid for reading
                            src_window = src_window.round_offsets().round_lengths()
                            data = src.read(
                                1,
                                window=src_window,
                                boundless=True,
                                fill_value=src.nodata,
                            )

                            if data is None or data.size == 0:
                                continue

                            # Create mask of valid data (not nodata)
                            src_nodata = src.nodata if src.nodata is not None else None
                            valid_mask = (
                                np.ones_like(data, dtype=bool)
                                if src_nodata is None
                                else data != src_nodata
                            )

                            if fim_type == "extent":
                                # For extent type, convert all non-zero values to 1
                                # But only for valid (not nodata) cells
                                data_temp = np.zeros_like(data, dtype=np.uint8)
                                # Set 1 where valid and non-zero
                                data_temp[valid_mask & (data != 0)] = 1
                                data = data_temp

                            # Update the output array - only where valid data exists
                            if np.any(valid_mask):
                                has_valid_data = True
                                # Only keep maximum values where mask is True
                                temp = np.full_like(data, nodata)
                                temp[valid_mask] = data[valid_mask]

                                # Create a mask for where out_data is nodata
                                out_nodata_mask = out_data == nodata

                                # Where both are valid, take max. Where only temp is valid, take temp.
                                # Where only out_data is valid, keep out_data
                                mask_both_valid = valid_mask & ~out_nodata_mask
                                mask_only_temp_valid = valid_mask & out_nodata_mask

                                if np.any(mask_both_valid):
                                    out_data[mask_both_valid] = np.maximum(
                                        out_data[mask_both_valid], temp[mask_both_valid]
                                    )
                                if np.any(mask_only_temp_valid):
                                    out_data[mask_only_temp_valid] = temp[
                                        mask_only_temp_valid
                                    ]

                        except Exception as e:
                            print(f"Warning: Error reading window from source: {e}")
                            continue

                    # For extent type, ensure we only have 0, 1, or nodata in the output
                    if fim_type == "extent" and has_valid_data:
                        valid_mask = out_data != nodata
                        out_data[valid_mask & (out_data > 0)] = 1

                    # Write block to output
                    dst.write(out_data, window=window, indexes=1)

            # Apply clipping if geometry provided
            if clip_geometry is not None:
                with rasterio.open(output_path, "r+") as src:
                    if isinstance(clip_geometry, str):
                        with fiona.open(clip_geometry, "r") as clip_file:
                            geoms = [feature["geometry"] for feature in clip_file]
                    else:
                        geoms = (
                            [clip_geometry]
                            if isinstance(clip_geometry, dict)
                            else clip_geometry
                        )

                    out_data, out_transform = mask(
                        src, geoms, crop=False, nodata=nodata
                    )
                    src.write(out_data[0], indexes=1)

    finally:
        # Close all source files
        for src in src_files:
            src.close()

    return output_path


if __name__ == "__main__":
    """
    Entry point to the mosaic_rasters function that mosaics a list of FIM extents or depths together.
    """

    parser = argparse.ArgumentParser(
        description="Mosaic multiple rasters with optional clipping.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument(
        "raster_paths",
        nargs="+",
        type=Path,
        help="Paths to the input rasters to be mosaicked",
    )

    parser.add_argument(
        "output_path", type=Path, help="Path where to save the mosaicked output"
    )

    parser.add_argument(
        "--clip-geometry",
        type=Path,
        help="Optional path to vector file for clipping the output",
    )

    parser.add_argument(
        "--fim-type",
        choices=["depth", "extent"],
        default="depth",
        help="Type of FIM output (affects data type and nodata value)",
    )

    parser.add_argument(
        "--geo-mem-cache",
        type=int,
        default=256,
        help="GDAL cache size in megabytes",
    )

    args = parser.parse_args()

    try:
        # Convert Path objects to strings for the main function
        output_raster = mosaic_rasters(
            raster_paths=[str(p) for p in args.raster_paths],
            output_path=str(args.output_path),
            clip_geometry=str(args.clip_geometry) if args.clip_geometry else None,
            fim_type=args.fim_type,
            geo_mem_cache=args.geo_mem_cache,
        )
        print(f"Successfully created mosaic: {output_raster}")

    except Exception as e:
        print(f"Error: {str(e)}", file=sys.stderr)
        sys.exit(1)
# begin fim_mosaicker/test/__init__.py
# begin fim_mosaicker/test/mock_data/make_test_mosaic_data.py
from osgeo import gdal, osr
import numpy as np

# Configuration
output_files = ["raster1.tif", "raster2.tif", "raster3.tif", "raster4.tif"]
nodata_values = [255, 254, 253, 252]  # Different nodata for each raster
corner_tiles = [
    {"row": (0, 256), "col": (0, 256)},  # Top-left corner
    {"row": (0, 256), "col": (512, 768)},  # Top-right corner
    {"row": (512, 768), "col": (0, 256)},  # Bottom-left corner
    {"row": (512, 768), "col": (512, 768)},  # Bottom-right corner
]

# Define spatial reference system (using WGS 84 as an example)
srs = osr.SpatialReference()
srs.ImportFromEPSG(4326)  # WGS 84

# Define geotransform parameters
# Format: (top_left_x, pixel_width, rotation, top_left_y, rotation, pixel_height)
# Example: Starting at longitude 0, latitude 45, with 0.001-degree pixel size
x_min = 0.0
y_max = 45.0
pixel_size = 0.001  # approximately 111 meters at the equator
geotransform = (x_min, pixel_size, 0, y_max, 0, -pixel_size)

for i in range(4):
    # Create a 768x768 array initialized with zeros
    data = np.zeros((768, 768), dtype=np.uint8)

    # Set the center tile to nodata
    data[256:512, 256:512] = nodata_values[i]

    # Set the designated corner to 1
    corner = corner_tiles[i]
    data[corner["row"][0] : corner["row"][1], corner["col"][0] : corner["col"][1]] = 1

    # Create the GeoTIFF with 256x256 tiling
    driver = gdal.GetDriverByName("GTiff")
    ds = driver.Create(
        output_files[i],
        768,
        768,
        1,
        gdal.GDT_Byte,
        options=["TILED=YES", "BLOCKXSIZE=256", "BLOCKYSIZE=256"],
    )

    # Set the geotransform and spatial reference
    ds.SetGeoTransform(geotransform)
    ds.SetProjection(srs.ExportToWkt())

    # Write data and set nodata value
    ds.GetRasterBand(1).WriteArray(data)
    ds.GetRasterBand(1).SetNoDataValue(nodata_values[i])

    # Close the dataset
    ds = None

print("Georeferenced datasets created successfully.")
# begin fim_mosaicker/test/test_mosaic.py
#!/usr/bin/env python3
import unittest
import os
import subprocess
import rasterio
import numpy as np
import sys


class TestMosaicScript(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        # Get the directory containing test data
        cls.test_dir = os.path.dirname(os.path.abspath(__file__))
        cls.mock_data_dir = os.path.join(cls.test_dir, "mock_data")

        # Ensure mock_data directory exists
        os.makedirs(cls.mock_data_dir, exist_ok=True)

        # Path to the script being tested
        cls.script_path = os.path.join(os.path.dirname(cls.test_dir), "mosaic.py")

        # Verify script exists
        if not os.path.exists(cls.script_path):
            raise FileNotFoundError(f"Script not found at {cls.script_path}")

        # Input raster paths (with .tif extension)
        cls.raster_paths = [
            os.path.join(cls.mock_data_dir, "raster1.tif"),
            os.path.join(cls.mock_data_dir, "raster2.tif"),
            os.path.join(cls.mock_data_dir, "raster3.tif"),
            os.path.join(cls.mock_data_dir, "raster4.tif"),
        ]

        # Output path
        cls.output_path = os.path.join(cls.mock_data_dir, "mosaicked_raster.tif")

        # Check for input files (print warning rather than failing setup)
        missing_files = []
        for raster_path in cls.raster_paths:
            if not os.path.exists(raster_path):
                missing_files.append(raster_path)

        if missing_files:
            print(
                f"WARNING: The following test raster files are missing: {missing_files}"
            )
            print("Tests may fail if these files are required.")

    def test_mosaic_creation(self):
        # Skip test if any input files are missing
        for raster_path in self.raster_paths:
            if not os.path.exists(raster_path):
                self.skipTest(f"Skipping test because {raster_path} is missing")

        # Remove the output file if it exists
        if os.path.exists(self.output_path):
            os.remove(self.output_path)

        # Run the mosaic script with FIM type set to extent
        cmd = [
            sys.executable,  # Use the current Python interpreter
            self.script_path,
            *self.raster_paths,
            self.output_path,
            "--fim-type",
            "extent",
        ]

        # Print the command being run
        print(f"Running command: {' '.join(cmd)}")

        # Run with full output capture
        result = subprocess.run(cmd, capture_output=True, text=True)

        # Print full output for debugging
        print(f"STDOUT: {result.stdout}")
        print(f"STDERR: {result.stderr}")

        self.assertEqual(
            result.returncode, 0, f"Script failed with error: {result.stderr}"
        )

        # Check if the output file was created
        self.assertTrue(
            os.path.exists(self.output_path), "Mosaic output file was not created."
        )

        # Verify the output raster properties
        with rasterio.open(self.output_path) as src:
            # Check data type (uint8 for extent)
            self.assertEqual(
                src.dtypes[0],
                "uint8",
                f"Expected uint8 data type for extent, got {src.dtypes[0]}",
            )

            # # Check nodata value (255 for extent)
            self.assertEqual(
                src.nodata,
                255,
                f"Expected 255 nodata value for extent, got {src.nodata}",
            )

            # Check that data exists
            data = src.read(1)
            self.assertTrue(
                (data != src.nodata).any(),
                "Raster contains no valid data (all nodata values)",
            )

            # For extent type, check that all values are either 0, 1 or nodata
            valid_data = data[data != src.nodata]
            if len(valid_data) > 0:
                self.assertTrue(
                    np.all((valid_data == 0) | (valid_data == 1)),
                    "Extent raster contains values other than 0, 1, and nodata",
                )

            # Check that there are 1s in the output (corners should be 1)
            self.assertTrue(
                np.any(data == 1),
                "Extent raster doesn't contain any 1 values, but should have corner tiles with 1s",
            )


if __name__ == "__main__":
    unittest.main()
# begin interfaces/fim_mosaicker.yml
id: fim_mosaicker
description: Mosaics and homogenizes overlapping raster flood observations into a combined dataset with optional clipping/reprojection
outputTransmission:
  - reference # Indicates output metadata/path is transmitted, not the full data 

arguments:

  fim_type:
    title: FIM Type
    description: Type of flood inundation map output
    schema:
      type: string
      enum: [depth, extent]
      default: depth

inputs:
  raster_paths:
    title: Input Raster Paths
    description: Array of paths/URIs to input raster flood observations (e.g., GeoTIFFs).
    required: true
    schema:
      type: array
      items:
        type: string
        format: uri
        contentMediaType: image/tiff; application=geotiff
      minItems: 1 

  clip_geometry_path:
    title: Clipping Geometry Path
    description: Path/URI to a vector file (e.g., GeoJSON, GPKG) containing polygon(s) for clipping the output mosaic.
    required: false 
    schema:
      type: string
      format: uri

outputs:
  mosaic_output_path:
    title: Mosaicked Dataset Path
    description: Path for the unified flood observation output (Raster GeoTIFF or Vector GeoPackage, EPSG:5070).
    schema:
      oneOf: 
        - type: string
          format: uri 
          contentMediaType: image/tiff; application=geotiff
          properties: 
            crs:
              const: EPSG:5070
              description: Standard US analysis projection with equal area preservation
            units: meters
            dtype: uint8        
            nodata: 255         
            compression: lzw    
          when: 
            properties:
              fim_type:
                const: extent

        - type: string
          format: uri 
          contentMediaType: image/tiff; application=geotiff
          properties: 
            crs:
              const: EPSG:5070
              description: Standard US analysis projection with equal area preservation
            units: meters
            dtype: float32      
            nodata: -9999       
            compression: lzw    
          when: 
            properties:
              fim_type:
                const: depth
